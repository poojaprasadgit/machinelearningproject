Radiology report generation (RRG) has drawn more and more research interest due to its enormous potential to alleviate the shortage of medical resources and support radiologists' illness decision-making. Few research explicitly examine the cross-modal alignment between picture regions and words, whereas recent advances in RRG are mostly motivated by enhancing a model's ability to encode single-modal feature representations. Cross-modal alignment is crucial to learning an RRG model that is aware of abnormalities in the picture since radiologists usually concentrate on problematic image regions before writing the related text descriptions. Inspired by this, we present a Class Activation Map guided Attention Network (CAMANet) that uses aggregated class activation maps to oversee cross-modal attention learning and concurrently enhance cross-modal alignment.
